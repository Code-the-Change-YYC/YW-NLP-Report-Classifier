{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J--poWga3Oag"
   },
   "source": [
    "# Report Description Classification\n",
    "\n",
    "This notebook compares the performance of report classification with preprocessing by Spacy and NLTK, and Complement Naive Bayes and SVM models.\n",
    "\n",
    "> This notebook is based off Bo's [ski_learn_with_spacy_finetune.ipynb](https://github.com/Code-the-Change-YYC/YW-NLP-Report-Classifier/blob/02ff7a9e7f49779c736cbb55edb4e8d2835beddd/notebooks/machine_learning/ski_learn_with_spacy_finetune.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Specification\n",
    "\n",
    "This notebook was tested with data preprocessed by `ReportData` with 335 training examples.\n",
    "\n",
    "> Lemmatization in preprocessing **is not used** as it is performed in the notebook.\n",
    "\n",
    "Commit tested at: `a9ed0b8b4587410fd969bce6481057b205d9049e`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "The results of multiple preprocessing combinations are summarized here:\n",
    "\n",
    "![image.png](./images/description_classification_results_no_weights.png)\n",
    "> Placeholder used is `'someone'`. w/o placeholders for scrubadub uses `'{{}}'` entities, and for spacy it uses `'*'` entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "If running this notebook in Google Colab, upload the requested files and allow the dependencies to be installed. The file paths should be treated as relative to the colab file, create any folders necessary to satisfy the paths.\n",
    "\n",
    "Otherwise update the path to allow for necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "\n",
    "    required_files = [\n",
    "        \"requirements.txt\",\n",
    "        \"data-processed.csv\",\n",
    "        \"report_data.py\",\n",
    "        \"report_data_d.py\",\n",
    "        \"incident_types_d.py\",\n",
    "        \"training/description_classification/utils.py\",\n",
    "        \"training/description_classification/model_paths.py\",\n",
    "    ]\n",
    "    for file in required_files:\n",
    "        print(f\"Upload {file}\")\n",
    "        files.upload()\n",
    "\n",
    "    !pip install -r requirements.txt\n",
    "else:\n",
    "    from os import path\n",
    "\n",
    "    root = path.abspath(path.join(\"..\", \"..\"))\n",
    "    sys.path.append(root)\n",
    "\n",
    "    preprocess = path.join(root, \"preprocess\")\n",
    "    sys.path.append(preprocess)\n",
    "\n",
    "    incident_types = path.join(preprocess, \"incident_types\")\n",
    "    sys.path.append(incident_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwaD_fMA3Xrp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from incident_types_d import IncidentType\n",
    "from preprocess.report_data import ReportData\n",
    "from preprocess.report_data_d import ColName\n",
    "from training.description_classification import model_paths, utils\n",
    "\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZyHRCL91ClA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "NLTK and Spacy versions of preprocessing to remove stop words and non-letter tokens, as well as perform lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRoiYwgwUHNI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    yw_df = ReportData(out_file_path=\"data-processed.csv\").get_processed_data()\n",
    "else:\n",
    "    yw_df = ReportData().get_processed_data()[[ColName.DESC,ColName.INC_T1]]\n",
    "\n",
    "print(yw_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHYQ-ytQ4Fp6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Summarize the differences between Spacy and NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRoiYwgwUHNI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yw_clean = yw_df[ColName.DESC]\n",
    "\n",
    "print(\n",
    "    \"Spacy tokenization compared to NLTK tokenization on the same report description:\\n\"\n",
    ")\n",
    "spacy_tokens = utils.spacy_tokenizer(yw_clean[0])\n",
    "nltk_tokens = utils.nltk_tokenizer(yw_clean[0])\n",
    "print(\"Items in spacy_tokens but not in nltk_tokens:\")\n",
    "print([x for x in spacy_tokens if x not in nltk_tokens])\n",
    "print()\n",
    "print(\"Items in nltk_tokens but not in spacy_tokens:\")\n",
    "print([x for x in nltk_tokens if x not in spacy_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use tf-idf with our Spacy tokenizer to vectorize the data. Note:\n",
    "\n",
    "- We match single character alphanumeric words instead of the default minimum double character.\n",
    "- We use both uni-grams and bi-grams, this gives more features and preserves some possibly important ordering. See [here](https://scikit-learn.org/stable/modules/feature_extraction.html?highlight=tfidf#common-vectorizer-usage) for an example.\n",
    "- We set `min_df` to filter odd words that don't appear often. We only need to consider more common word patterns and filter out the odd words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N_M0ErRbfl3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_vec = TfidfVectorizer(\n",
    "    tokenizer=utils.spacy_tokenizer,\n",
    "    token_pattern=r\"\\b\\w+\\b\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIXfSYO_1vQC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data into training and test data. The `random_state` of `32` has been manually optimized for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gu7fF5PUZBS0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = yw_clean\n",
    "y = yw_df[ColName.INC_T1]\n",
    "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GyDpbswryUK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compute sample weights for each example, giving higher frequency examples more weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1hXOh43p9Zy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weight_all = compute_sample_weight(utils.count_weight(y), y)\n",
    "weight_train = compute_sample_weight(utils.count_weight(y_train_set), y_train_set)\n",
    "weight_test = compute_sample_weight(utils.count_weight(y_test_set), y_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl7MRO9j1zJq"
   },
   "source": [
    "## Training and Cross Validation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize and fit the models.\n",
    "> NOTE: Using sample weights with CNB significantly decreases the accuracy, this could be related to the inner workings of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIghMdHfZRHv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnb_cache = mkdtemp()\n",
    "cnb = make_pipeline(word_vec, CalibratedClassifierCV(ComplementNB(),method=\"sigmoid\"), memory=cnb_cache)\n",
    "cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIghMdHfZRHv"
   },
   "outputs": [],
   "source": [
    "svm_cache = mkdtemp()\n",
    "svm = make_pipeline(word_vec, SVC(), memory=svm_cache)\n",
    "svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YL0vW9q8DcTt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fine tune the estimator hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoded input as it takes too long to generate each grid search\n",
    "X_train_enc = word_vec.fit_transform(X_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlHPD44WRoUV"
   },
   "outputs": [],
   "source": [
    "# To save time some of the best options from previous runs are selected here\n",
    "# Once in a while this should be rerun with all options to ensure the best options haven't become outdated\n",
    "svc_params_list = {\n",
    "    \"C\": np.linspace(1, 0, num=5, endpoint=False),\n",
    "    \"coef0\": np.logspace(-1, 1, num=5),\n",
    "    \"kernel\": [\"sigmoid\"],  # [\"linear\", \"poly\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\"],  # [\"scale\", \"auto\"],\n",
    "    \"decision_function_shape\": [\n",
    "        \"ovo\"\n",
    "    ],  # Multi-class is always handled with one-vs-one # [\"ovo\", \"ovr\"],\n",
    "    \"class_weight\": [\"balanced\"],  # [\"balanced\", None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK48_p7__Bt5"
   },
   "outputs": [],
   "source": [
    "svm_op = GridSearchCV(svm.named_steps[\"svc\"], param_grid=svc_params_list)\n",
    "svm_op.fit(X_train_enc, y_train_set, sample_weight=weight_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZCkGNV3So0_"
   },
   "outputs": [],
   "source": [
    "svm_op.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1omVlVqgYJx6"
   },
   "outputs": [],
   "source": [
    "scoring = [\"recall_weighted\", \"precision_weighted\", \"balanced_accuracy\", \"accuracy\"]\n",
    "fit_params = {\"sample_weight\": weight_train}\n",
    "\n",
    "cv_s = cross_validate(\n",
    "    svm_op.best_estimator_,  # change to svm variable to see differences from fine tuning\n",
    "    X_train_enc,\n",
    "    y_train_set,\n",
    "    scoring=scoring,\n",
    "    fit_params=fit_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKIN__sJfXWO"
   },
   "outputs": [],
   "source": [
    "cnb_op = GridSearchCV(\n",
    "    cnb.named_steps[\"calibratedclassifiercv\"].base_estimator,\n",
    "    param_grid={\"alpha\": np.linspace(3, 0, num=50, endpoint=False)},\n",
    ")\n",
    "cnb_op.fit(X_train_enc, y_train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrkJ4F4bfs46",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnb_op.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlkIYCxkOZ34"
   },
   "outputs": [],
   "source": [
    "cv_b = cross_validate(cnb_op.best_estimator_, X_train_enc, y_train_set, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation results (training set only)."
   ]
  },
  {
   "source": [
    "metrics = cv_b.keys()\n",
    "names = [\"Metric\", \"Model\"]\n",
    "index = pd.MultiIndex.from_product([metrics, [\"CNB\", \"SVM\"]], names=names)\n",
    "pairs = zip(cv_b.values(), cv_s.values())\n",
    "flattened = sum(pairs, ())\n",
    "df = pd.DataFrame(flattened, index=index)\n",
    "df.join(df.agg(func=[\"mean\", \"max\"], axis=1), on=names)"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test out calibrated classifier\n",
    "encoded_desc = word_vec.fit_transform(X)\n",
    "encoder = LabelEncoder()\n",
    "encodedY = encoder.fit_transform(y)\n",
    "cnb_op.fit(encoded_desc,encodedY)\n",
    "\n",
    "\n",
    "cal_clf_sig = CalibratedClassifierCV(cnb_op,cv=\"prefit\",method=\"sigmoid\")\n",
    "cal_clf_iso = CalibratedClassifierCV(cnb_op,cv=\"prefit\",method=\"isotonic\")\n",
    "cal_clf_sig.fit(encoded_desc,encodedY)\n",
    "cal_clf_iso.fit(encoded_desc,encodedY)\n",
    "\n",
    "\n",
    "prob_cnb_sig = cal_clf_sig.predict_proba(encoded_desc)\n",
    "prob_cnb_iso = cal_clf_iso.predict_proba(encoded_desc)\n",
    "prob_cnb_old =cnb_op.predict_proba(encoded_desc) \n",
    "\n",
    "cnb_score_sig =  log_loss(encodedY, prob_cnb_sig)\n",
    "cnb_score_iso =  log_loss(encodedY, prob_cnb_iso)\n",
    "cnb_score_old = log_loss(encodedY,prob_cnb_old)\n",
    "\n",
    "\n",
    "print(\"With sigmoid calibration: %1.3f\" % cnb_score_sig)\n",
    "print(\"With isotonic calibration: %1.3f\" % cnb_score_iso)\n",
    "print(\"No calibration: %1.3f \\n\" % cnb_score_old)\n",
    "\n",
    "# print(\"***************Showing caliberated effects for first 5 datasets***************\")\n",
    "for (i,desc) in enumerate(encodedY[:5]):\n",
    "  print(f\"encoded index : {desc} and class label : {encoder.inverse_transform([desc])[0]}\")\n",
    "  print(f\"No calibration predict proba: {prob_cnb_old[i][desc]:.2f} at index {desc}\")\n",
    "  print(f\"With sigmoid calibration predict proba: {prob_cnb_sig[i][desc]:.2f} at index {desc}\")\n",
    "  print(f\"With isotonic calibration predict proba: {prob_cnb_iso[i][desc]:.2f} at index {desc}\\n\")\n",
    "  \n"
   ]
  },
  {
   "source": [
    "## Model Saving\n",
    "\n",
    "Save the optimized models to pickle files after retraining on the entire dataset. If just training the models you can run only up to this cell.\n",
    "> NOTE: In Colab the files will be saved to the current directory, download these to your local `model_output` folder."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_output_path(file_name: str, full_path: str) -> str:\n",
    "    return full_path if not IN_COLAB else f\"./{file_name}\"\n",
    "\n",
    "\n",
    "def save_model(model, file_name: str, full_path: str):\n",
    "    with open(get_model_output_path(file_name, full_path), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "# Ensure each of the pipelines' estimators are using the best params\n",
    "for cv, pipe in [(svm_op, svm)]:\n",
    "    # The last step, the estimator comes after the name\n",
    "    estimator = pipe.steps[-1][1]\n",
    "    estimator.set_params(**cv.best_params_)\n",
    "for cv, pipe in [(cnb_op, cnb)]:\n",
    "    # The last step, the estimator comes after the name\n",
    "    estimator = pipe.steps[-1][1].base_estimator\n",
    "    estimator.set_params(**cv.best_params_)\n",
    "\n",
    "\n",
    "cnb.fit(X, y)\n",
    "\n",
    "\n",
    "save_model(cnb, model_paths.cnb_cli_file_name, model_paths.cnb_cli)\n",
    "#old path for cnb file.\n",
    "# save_model(cnb, model_paths.cnb_file_name, model_paths.cnb)\n",
    "\n",
    "# svm.fit(X, y, svc__sample_weight=weight_all)\n",
    "# save_model(svm, model_paths.svm_file_name, model_paths.svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the models on the entire training set. Click on the pipeline steps to view the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnb.fit(X_train_set, y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train_set, y_train_set, svc__sample_weight=weight_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> NOTE: Using `sample_weight=weight_test` in our metrics calculations makes samples which are more prevalent in our data contribute more to the overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GJYVI3TaH7W"
   },
   "outputs": [],
   "source": [
    "print(\"Complement NB train:\\n\")\n",
    "utils.show_classification_report(\n",
    "    cnb, X_train_set, y_train_set, sample_weight=weight_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GJYVI3TaH7W"
   },
   "outputs": [],
   "source": [
    "print(\"Complement NB test:\\n\")\n",
    "utils.show_classification_report(cnb, X_test_set, y_test_set, sample_weight=weight_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9b8dXW2Be3_f"
   },
   "outputs": [],
   "source": [
    "print(\"SVM-C train:\\n\")\n",
    "utils.show_classification_report(\n",
    "    svm, X_train_set, y_train_set, sample_weight=weight_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9b8dXW2Be3_f"
   },
   "outputs": [],
   "source": [
    "print(\"SVM-C test:\\n\")\n",
    "utils.show_classification_report(svm, X_test_set, y_test_set, sample_weight=weight_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nltk_vs_spacy_classification_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3610jvsc74a57bd06b65eab8ca55a3eb4279f5080b91c40aaef1937619015e7a792bc5b7c8fd3b93",
   "display_name": "Python 3.6.10 64-bit ('3.6.10')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "metadata": {
   "interpreter": {
    "hash": "6b65eab8ca55a3eb4279f5080b91c40aaef1937619015e7a792bc5b7c8fd3b93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}